---
title: "Lecture 15: Spatial Prediction and Model Selection"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(message = FALSE, warning = F)
library(tidyverse)
library(scoringRules)
```


## Conditional Multivariate Normal Theory
The conditional distribution, $p(\boldsymbol{Y_1}|
\boldsymbol{Y_2}, \boldsymbol{\beta}, \sigma^2, \phi, \tau^2)$ is normal with:

\vfill

- $E[\boldsymbol{Y_1}|
\boldsymbol{Y_2}] = \boldsymbol{\mu_1} + \Omega_{12} \Omega_{22}^{-1} (\boldsymbol{Y_2} - \mu_2)$

\vfill

- $Var[\boldsymbol{Y_1}|
\boldsymbol{Y_2}] = \Omega_{11} - \Omega_{12} \Omega_{22}^{-1} \Omega_{21}$


\vfill

## Posterior Predictive Distribution

The (posterior) predictive distribution $p(Y(\boldsymbol{s_0})|y)$ can be written as
$$p(Y(\boldsymbol{s_0})|y) = \int p(Y(\boldsymbol{s_0})|y, \boldsymbol{\theta})p( \boldsymbol{\theta}|y) d  \boldsymbol{\theta}$$
where  $\boldsymbol{\theta} = \{\boldsymbol{\beta}, \sigma^2, \phi, \tau^2\}$.

\vfill

The posterior predictive distribution gives a probabilistic forecast for the outcome of interest that does not depend on any unknown parameters.

\vfill

These are the individual samples for a specific point that we have seen in STAN.

\vfill

## Prediction
We will use cross-validation or a test/training approach to compare predictive models.

\vfill

Consider three data structures: continuous, count, and binary; how should we evaluate predictions in these situations?

\newpage

## Loss Functions
Loss functions penalize predictions that deviate from the true values.

\vfill

For continuous or count data, squared error loss and absolute error loss are common.

\vfill

With binary data, a zero-one loss is frequently used.

\vfill
However, these metrics are all focused on point estimates.

\vfill

If we think about outcomes distributionally, empirical coverage probability can be considered. For instance, our 95 % prediction intervals should, on average, have roughly 95 % coverage.

\vfill

With interval predictions, the goal is to have a concentrated predictive distribution around the outcome.

\vfill

The Continuous Rank Probability Score (CRPS) defined as 
$$CRPS(F,y) = \int_{-\infty}^{\infty}\left(F(u) - 1(u \geq y) \right)^2 du,$$
where $F$ is the CDF of the predictive distribution, is a metric that measures distance between an observed value and a distribution.

\newpage

```{r, echo = T}
crps_sample(y = 0, dat = 0)
crps_sample(y = 0, dat = 2)
crps_sample(y = 0, dat = c(2,-2))
crps_sample(y = 0, dat = c(2,0,-2))
crps_sample(y = 0, dat = c(2,0,0,0,0,-2))
```


## CRPS
Consider four situations and sketch the predictive distribution and the resultant CRPS for each scenario. How does the MSE function in each setting?

1. Narrow predictive interval centered around outcome.
\vfill

2. Wide predictive interval centered around outcome.

\vfill

3. Narrow predictive interval with outcome in tail.

\vfill

4. Wide predictive interval with outcome in tail.

\vfill

\newpage

\vfill
```{r}
y <- 0
sample <- rnorm(10000)
f1 <- tibble(data = sample) %>% ggplot(aes(x = data)) + 
  geom_histogram(color = 'grey', fill = 'grey', bins = 100) +
  theme_bw() + geom_vline(xintercept = 0) +
  annotate('text', label = paste("CRPS = ",round(crps_sample(y = y, dat = sample),2)), x = 0, y = 100) + xlim(-10,10)+ ylim(NA,1000)
```


```{r}
y <- 0
sample <- rnorm(10000, sd = 3)
f2 <- tibble(data = sample) %>% ggplot(aes(x = data)) + 
  geom_histogram(color = 'grey', fill = 'grey', bins = 100) +
  theme_bw() + geom_vline(xintercept = 0) +
  annotate('text', label = paste("CRPS = ",round(crps_sample(y = y, dat = sample),2)), x = 0, y = 100) + xlim(-10,10)+ ylim(NA,1000)
```
\vfill


```{r}
y <- 0
sample <- rnorm(10000, mean = -2)
f3 <- tibble(data = sample) %>% ggplot(aes(x = data)) + 
  geom_histogram(color = 'grey', fill = 'grey', bins = 100) +
  theme_bw() + geom_vline(xintercept = 0) +
  annotate('text', label = paste("CRPS = ",round(crps_sample(y = y, dat = sample),2)), x = 0, y = 100) + xlim(-10,10)+ ylim(NA,1000)
```


\vfill



```{r}
y <- 0
sample <- rnorm(10000, mean = -2, sd = 3)
f4 <- tibble(data = sample) %>% ggplot(aes(x = data)) + 
  geom_histogram(color = 'grey', fill = 'grey', bins = 100) +
  theme_bw() + geom_vline(xintercept = 0) +
  annotate('text', label = paste("CRPS = ",round(crps_sample(y = y, dat = sample),2)), x = 0, y = 100) + xlim(-10,10) + ylim(NA,1000)
```

\vfill

```{r}
gridExtra::grid.arrange(f1, f2, f3, f4)
```

